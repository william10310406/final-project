# 專案程式碼詳解

### 文件目的
本文件旨在提供對「Flask 部落格與捷運資訊系統」專案的深入程式碼解釋，幫助您理解系統的架構、設計模式以及各個模組的具體實現。

---

### 1. 核心 Web 應用程式詳解

本專案的核心 Web 應用遵循經典的 **MVC (Model-View-Controller)** 設計模式，將數據、業務邏輯與使用者介面清晰地分離。

- **Model (模型)**: `models.py` - 負責數據庫互動與業務邏輯。
- **View (視圖)**: `templates/` 目錄 - 負責數據的呈現與使用者介面。
- **Controller (控制器)**: `app.py` - 負責接收使用者請求、協調模型與視圖。

---

#### 1.1 `app.py` - 應用程式控制器

此檔案是專案的神經中樞，負責定義 URL 路由、處理使用者請求，並將最終結果呈現給使用者。

**設計理念**:
- **單一職責**: `app.py` 專注於流程控制。所有數據庫操作都委託給 `models.py`，所有頁面渲染都交給 `templates/`。
- **配置分離**: 敏感資訊（如資料庫 URI、密鑰）通過環境變數加載，實現了開發環境與生產環境的配置分離，提高了安全性與靈活性。

**關鍵程式碼詳解**:

1.  **初始化與配置 (L8-L24)**
    ```python
    # 從環境變數讀取配置，若無則使用預設值
    MONGODB_URI = os.environ.get(...) 
    SECRET_KEY = os.environ.get(...)
    
    # 初始化資料庫連接
    init_db(MONGODB_URI)
    
    # 創建並配置 Flask 應用
    app = Flask(__name__)
    app.secret_key = SECRET_KEY
    ```
    - **數據流**: 應用程式啟動時，首先從環境變數讀取配置，然後調用 `models.py` 的 `init_db` 函數建立與 MongoDB 的連接。這個順序至關重要，確保了在處理任何請求之前，資料庫已準備就緒。

2.  **`login_required` 裝飾器 (L27-L36)**
    - **目的**: 實現路由保護，確保只有登入的用戶才能訪問特定頁面（如創建文章）。
    - **機制**: 它檢查 `session` 中是否存在 `user_id`。若不存在，則中斷請求，並將用戶重定向到登入頁面。這是一個非常優雅的實現授權 (Authorization) 的方式。

3.  **使用者介面路由 (例如 `/post/<post_id>`) (L206-L220)**
    ```python
    @app.route('/post/<post_id>')
    def view_post(post_id):
        # 1. 調用模型，根據 ID 獲取文章數據
        post = Post.find_by_id(post_id)
        if post:
            # 2. 獲取關聯數據：作者和留言
            author = User.find_by_id(post.user_id)
            comments = Comment.find_by_post_id(post_id)
            # 3. 將所有數據傳遞給視圖進行渲染
            return render_template('view_post.html', ...)
    ```
    - **數據流**:
        1.  控制器 (`app.py`) 接收到請求。
        2.  控制器調用模型 (`models.py`) 獲取主數據 (文章) 和關聯數據 (作者、留言)。
        3.  模型與資料庫通訊，返回數據給控制器。
        4.  控制器將數據打包，傳遞給視圖 (`view_post.html`)。
        5.  視圖使用 Jinja2 模板引擎將數據渲染成最終的 HTML 頁面，返回給使用者。

4.  **API 路由 (例如 `/api/mrt/stream`) (L195-L204)**
    - **目的**: 為前端提供純數據接口，實現頁面非同步更新。
    - **機制**: 這類路由不返回 HTML，而是調用 `jsonify()` 將從模型獲取的 Python 對象序列化為 JSON 格式。前端的 JavaScript 可以通過 `fetch` 或 `AJAX` 請求這些 API，獲取最新數據並動態更新圖表，而無需重新加載整個頁面。

---

#### 1.2 `models.py` - 數據模型層

此檔案是專案的數據基礎，封裝了所有與 MongoDB 的互動細節，並以物件導向的方式定義了應用程式的業務實體。

**設計理念**:
- **物件關係對映 (ORM) 的簡化實現**: 每個類別 (如 `User`, `Post`) 對應到資料庫中的一個集合 (Collection)。類別的方法 (如 `save`, `find_all`) 封裝了對該集合的 CRUD (創建、讀取、更新、刪除) 操作。
- **數據與邏輯的封裝**: 將數據驗證（如 `check_password`）和業務邏輯（如 `find_by_user_id`）與數據本身綁定在一起，使得 `app.py` 的程式碼更簡潔、更專注於流程控制。

**關鍵程式碼詳解**:

1.  **延遲初始化模式 (L22-L27)**
    - **問題**: 直接在 Python 模組的頂層（類別外部）建立資料庫連接，可能會因為導入順序問題導致在連接建立前就被使用。
    - **解決方案**: `db` 變數初始為 `None`。通過 `init_db()` 函數，由 `app.py` 在最合適的時機（應用程式啟動時）明確地進行初始化。

2.  **`get_collection()` 類別方法 (L33-L36)**
    - **問題**: PyMongo 的 `db` 和 `collection` 物件不支持直接的布林值測試 (e.g., `if db:` 會拋出錯誤)。
    - **解決方案**: 我們創建一個類別方法，使用 `is None` 來進行明確的 `None` 檢查。所有需要訪問資料庫的方法都必須先調用此方法來安全地獲取集合物件。這是一個健壯性 (Robustness) 的體現。
      ```python
      @classmethod
      def get_collection(cls):
          return None if db is None else db.users
      ```

3.  **密碼安全性 (L50, L86-L90)**
    - **機制**:
        - **儲存**: `generate_password_hash(password)`: 將用戶的明文密碼添加「鹽值 (salt)」後進行多次雜湊，生成一個不可逆的雜湊值存入資料庫。
        - **驗證**: `check_password_hash(hash, password)`: 將用戶登入時輸入的密碼用同樣的方式處理，並與資料庫中儲存的雜湊值進行比對。
    - **重要性**: 這種方式確保了即使資料庫被洩漏，攻擊者也無法直接獲取用戶的原始密碼。

---

### 2. 數據採集引擎 (`crawler/`)

此目錄下的腳本是獨立的數據採集程式，負責從「臺北市政府資料大平臺」獲取捷運的即時數據。

**設計理念**:
- **關注點分離**: 將數據採集與 Web 應用分離。爬蟲腳本可以通過系統的排程任務 (如 Linux 的 cron job) 定期執行，而不會影響主 Web 應用的運行。
- **數據持久化**: 爬蟲將獲取的原始數據儲存為 CSV 檔案，作為數據的本地備份和後續處理的數據源。

**注意**: 當前架構中，從 CSV 檔案到 MongoDB 資料庫的導入過程需要一個額外的腳本來完成，這可作為專案的「未來工作」。Web 應用直接讀取的是 MongoDB 中的數據。

---

#### 2.1 `crawler/stream.py` - 捷運總人流爬蟲

- **目的**: 獲取整個捷運系統在當天的累計人流數據。
- **數據流**:
    1.  `crawl_data()`: 動態構建 API URL，包含當天的起訖時間。
    2.  `requests.get()`: 發送 HTTP 請求獲取 JSON 數據。
    3.  `parser.isoparse()`: 解析 API 回傳的 ISO 8601 格式時間戳。
    4.  `save_to_csv()`: 將處理過的數據**覆寫**到以當天日期命名的 CSV 檔案中。

---

#### 2.2 `crawler/carriage.py` - 各線車廂擁擠度爬蟲

- **目的**: 獲取各條捷運線、各個車站、各個方向的車廂擁擠度。這比總人流爬蟲要複雜得多。
- **數據流**:
    1.  讀取 `MRT_line.json` 獲取各路線的 API ID。
    2.  `crawl_data()`: 遍歷所有路線，發送請求獲取數據。
    3.  `merge_data()`: **此為本腳本最核心的邏輯**。API 返回的數據是扁平化的列表，需要與 `MRT_line.json` 中定義的站點資訊進行合併，才能構建出結構化的數據（站名、往站點A方向擁擠度、往站點B方向擁擠度）。
    4.  **特殊邏輯處理**: 特別處理了橘線（中和新蘆線），因為它有迴龍、蘆洲兩個分支，需要分別處理和合併。
    5.  `save_to_csv()`: 將處理好的結構化數據**追加**到以路線和日期命名的 CSV 檔案中，並在每次寫入前添加時間戳。

**潛在改進**:
- **硬編碼路徑**: 腳本中的檔案路徑 (`/Users/angelina1114/...`) 是寫死的，這降低了程式碼的可移植性。更好的做法是使用相對路徑或通過配置檔案來管理路徑。 